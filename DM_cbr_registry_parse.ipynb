{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e297709",
   "metadata": {},
   "source": [
    "The code is a Python script used for web scraping the registries data from the Central Bank of Russia website. It uses the xlwt library to create an Excel workbook and BeautifulSoup and Selenium to parse and interact with the website's HTML source code. After navigating to the desired URL, the code clicks a \"Load more\" button several times to load more data onto the page. It then searches the page for all div elements with a class of \"title-source offset-md-4\" to find the desired data, and saves the titles and URLs to separate sheets in the workbook. Finally, the script saves the workbook to a file named \"test_New.xls\". The resulting Excel file provides the scraped registry data in a ready-to-analyze format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "book = xlwt.Workbook(encoding=\"utf-8\")\n",
    "\n",
    "sheet1 = book.add_sheet(\"Sheet 1\")\n",
    "sheet2 = book.add_sheet(\"Sheet 2\")\n",
    "\n",
    "url = 'https://www.cbr.ru/registries/'\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(3)\n",
    "driver.get (url)\n",
    "button = driver.find_element_by_xpath(\"//button[@class='btn _load cross-load-more']\")\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "button.click()\n",
    "#button.click()\n",
    "\n",
    "#response = requests.get (url)\n",
    "#soup = BeautifulSoup (response.text, 'lxml')\n",
    "soup = BeautifulSoup (driver.page_source, 'lxml')\n",
    "titles = soup.find_all('div', class_='title-source offset-md-4')\n",
    "\n",
    "p = 4\n",
    "\n",
    "for n, i in enumerate(titles, start=1):\n",
    "    titleName = i.find('div', class_='title').text.strip()\n",
    "    p = p+1\n",
    "    sheet1.write(p, 0, titleName)\n",
    "    #print (titleName)\n",
    "\n",
    "for link in soup.findAll('a'):\n",
    "    p = p+1\n",
    "    y = link.get('href')\n",
    "    sheet2.write(p, 0, y)\n",
    "    #print(link.get('href'))\n",
    "    \n",
    "book.save(\"test_New.xls\")\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit ('3.10.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "be39b3f8db6f6aadae59bee8ccdccf8c340e7c693887d133fde64e2e9d5b4ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
